{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/Escritorio/Inteligencia_Artificial/Inteligencia Artificial/proyecto/Proyecto_IA_Turistica/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-07 00:32:56,927 - INFO - Inicio del notebook 'rag.ipynb'.\n",
      "2025-10-07 00:32:56,929 - INFO - Dispositivo seleccionado para los cálculos: cpu\n"
     ]
    }
   ],
   "source": [
    "# [1]\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import google.generativeai as genai\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# --- Configuración del Logging ---\n",
    "log_file = 'rag.log'\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"Inicio del notebook 'rag.ipynb'.\")\n",
    "\n",
    "# --- Constantes y Rutas ---\n",
    "# Definimos las rutas a los artefactos que creamos en el notebook anterior.\n",
    "DATA_DIR = '../../datos_limpios/'\n",
    "EMBEDDINGS_PATH = os.path.join(DATA_DIR, 'embeddings.pt')\n",
    "JSON_PATH = os.path.join(DATA_DIR, 'sitios_cali_maestro.json')\n",
    "MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Variable para controlar el dispositivo (GPU si está disponible, si no CPU)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\"Dispositivo seleccionado para los cálculos: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:11:00,788 - INFO - Tensor de embeddings cargado desde '../../datos_limpios/embeddings.pt' con forma: torch.Size([2470, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:11:00,889 - INFO - Se cargaron 2470 registros de sitios desde '../../datos_limpios/sitios_cali_maestro.json'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de conocimiento cargada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# [2]\n",
    "# --- Carga de la Base de Conocimiento ---\n",
    "\n",
    "try:\n",
    "    # 1. Cargar el tensor de embeddings\n",
    "    # .to(DEVICE) mueve el tensor a la GPU si está disponible, acelerando los cálculos.\n",
    "    corpus_embeddings = torch.load(EMBEDDINGS_PATH, map_location=DEVICE)\n",
    "    logging.info(f\"Tensor de embeddings cargado desde '{EMBEDDINGS_PATH}' con forma: {corpus_embeddings.shape}\")\n",
    "\n",
    "    # 2. Cargar la lista de sitios desde el archivo JSON\n",
    "    with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        corpus_sitios = json.load(f)\n",
    "    logging.info(f\"Se cargaron {len(corpus_sitios)} registros de sitios desde '{JSON_PATH}'.\")\n",
    "\n",
    "    print(\"Base de conocimiento cargada exitosamente.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"No se pudo encontrar un archivo necesario: {e}\")\n",
    "    print(f\"Error: No se encontró el archivo '{e.filename}'. Asegúrate de haber ejecutado el notebook 'embedding.ipynb' primero.\")\n",
    "    corpus_embeddings, corpus_sitios = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:11:00,903 - INFO - Cargando el modelo codificador 'paraphrase-multilingual-mpnet-base-v2'...\n",
      "2025-10-07 00:11:00,908 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
      "2025-10-07 00:11:00,908 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
      "2025-10-07 00:11:05,373 - INFO - Modelo codificador cargado exitosamente.\n",
      "2025-10-07 00:11:05,373 - INFO - Modelo codificador cargado exitosamente.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo codificador cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# [3]\n",
    "# --- Carga del Modelo Codificador (Encoder) ---\n",
    "# Debe ser exactamente el mismo modelo usado para crear los embeddings del corpus.\n",
    "\n",
    "try:\n",
    "    logging.info(f\"Cargando el modelo codificador '{MODEL_NAME}'...\")\n",
    "    model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "    logging.info(\"Modelo codificador cargado exitosamente.\")\n",
    "    print(\"Modelo codificador cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    logging.critical(f\"No se pudo cargar el modelo de SentenceTransformer. Error: {e}\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4]\n",
    "# --- Función del Retriever (Búsqueda de Similitud) ---\n",
    "\n",
    "def buscar_sitios_relevantes(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Toma una consulta de usuario, la convierte en un embedding y encuentra los 'top_k' sitios más similares.\n",
    "    \"\"\"\n",
    "    if corpus_embeddings is None or model is None:\n",
    "        logging.error(\"La base de conocimiento o el modelo no están cargados. No se puede realizar la búsqueda.\")\n",
    "        return []\n",
    "\n",
    "    logging.info(f\"Recibida nueva consulta: '{query}'\")\n",
    "    \n",
    "    # 1. Codificar la consulta del usuario en un vector (embedding)\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=DEVICE)\n",
    "    \n",
    "    # 2. Calcular la similitud del coseno entre la consulta y todos los embeddings del corpus\n",
    "    # util.cos_sim es una operación de tensor altamente optimizada.\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    # 3. Encontrar los 'top_k' resultados con las puntuaciones más altas\n",
    "    # torch.topk nos devuelve tanto las puntuaciones como los índices de los mejores resultados.\n",
    "    top_results = torch.topk(cos_scores, k=min(top_k, len(corpus_sitios)))\n",
    "    \n",
    "    logging.info(f\"Búsqueda completada. Se encontraron {len(top_results[0])} resultados relevantes.\")\n",
    "    \n",
    "    # 4. Formatear y devolver los resultados\n",
    "    resultados = []\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        # idx es la posición del sitio en nuestra lista original 'corpus_sitios'\n",
    "        sitio_encontrado = corpus_sitios[idx]\n",
    "        resultados.append({\n",
    "            \"score\": score.item(), # Convertimos el tensor de la puntuación a un número simple\n",
    "            \"nombre\": sitio_encontrado.get(\"nombre\"),\n",
    "            \"direccion\": sitio_encontrado.get(\"direccion\"),\n",
    "            \"comentarios\": sitio_encontrado.get(\"comentarios\")\n",
    "        })\n",
    "        \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:11:05,410 - INFO - Recibida nueva consulta: 'Quiero ir a un lugar romántico para una cita, que sea elegante y con comida italiana.'\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
      "2025-10-07 00:11:05,622 - INFO - Búsqueda completada. Se encontraron 5 resultados relevantes.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
      "2025-10-07 00:11:05,622 - INFO - Búsqueda completada. Se encontraron 5 resultados relevantes.\n",
      "2025-10-07 00:11:05,627 - INFO - Recibida nueva consulta: 'Un sitio para caminar y ver arte o historia de la ciudad'\n",
      "2025-10-07 00:11:05,627 - INFO - Recibida nueva consulta: 'Un sitio para caminar y ver arte o historia de la ciudad'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados para la consulta: 'Quiero ir a un lugar romántico para una cita, que sea elegante y con comida italiana.' ---\n",
      "  - Puntuación: 0.7055 | Nombre: storia damore granada\n",
      "  - Puntuación: 0.6685 | Nombre: afuego mgico\n",
      "  - Puntuación: 0.6475 | Nombre: piazza by storia damore unicentro cali\n",
      "  - Puntuación: 0.6418 | Nombre: restaurante casa mar\n",
      "  - Puntuación: 0.6413 | Nombre: la over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "2025-10-07 00:11:05,816 - INFO - Búsqueda completada. Se encontraron 5 resultados relevantes.\n",
      "2025-10-07 00:11:05,818 - INFO - Recibida nueva consulta: 'dónde puedo comer la mejor comida típica del valle, como empanadas o lulada'\n",
      "2025-10-07 00:11:05,816 - INFO - Búsqueda completada. Se encontraron 5 resultados relevantes.\n",
      "2025-10-07 00:11:05,818 - INFO - Recibida nueva consulta: 'dónde puedo comer la mejor comida típica del valle, como empanadas o lulada'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados para la consulta: 'Un sitio para caminar y ver arte o historia de la ciudad' ---\n",
      "  - Puntuación: 0.7499 | Nombre: monumento letras de cali\n",
      "  - Puntuación: 0.7323 | Nombre: museo h tejada\n",
      "  - Puntuación: 0.7164 | Nombre: lugar a dudas\n",
      "  - Puntuación: 0.7124 | Nombre: calle de la escopeta\n",
      "  - Puntuación: 0.7103 | Nombre: parque mirador yo amo a silo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "2025-10-07 00:11:06,049 - INFO - Búsqueda completada. Se encontraron 5 resultados relevantes.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "2025-10-07 00:11:06,049 - INFO - Búsqueda completada. Se encontraron 5 resultados relevantes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados para la consulta: 'dónde puedo comer la mejor comida típica del valle, como empanadas o lulada' ---\n",
      "  - Puntuación: 0.7389 | Nombre: chicharritos antojos de tradicin\n",
      "  - Puntuación: 0.7043 | Nombre: fritanga las collazos\n",
      "  - Puntuación: 0.6867 | Nombre: antojitos tipicos del valle restaurante cali norte\n",
      "  - Puntuación: 0.6732 | Nombre: los antojos de yayis\n",
      "  - Puntuación: 0.6716 | Nombre: ricuras de la 12\n"
     ]
    }
   ],
   "source": [
    "# [5]\n",
    "# --- Simulación de Consultas de Usuario ---\n",
    "\n",
    "# Consulta 1: Específica y con sentimiento\n",
    "consulta_1 = \"Quiero ir a un lugar romántico para una cita, que sea elegante y con comida italiana.\"\n",
    "resultados_1 = buscar_sitios_relevantes(consulta_1)\n",
    "print(f\"\\n--- Resultados para la consulta: '{consulta_1}' ---\")\n",
    "for res in resultados_1:\n",
    "    print(f\"  - Puntuación: {res['score']:.4f} | Nombre: {res['nombre']}\")\n",
    "\n",
    "# Consulta 2: Basada en actividades\n",
    "consulta_2 = \"Un sitio para caminar y ver arte o historia de la ciudad\"\n",
    "resultados_2 = buscar_sitios_relevantes(consulta_2)\n",
    "print(f\"\\n--- Resultados para la consulta: '{consulta_2}' ---\")\n",
    "for res in resultados_2:\n",
    "    print(f\"  - Puntuación: {res['score']:.4f} | Nombre: {res['nombre']}\")\n",
    "\n",
    "# Consulta 3: Gastronómica y popular\n",
    "consulta_3 = \"dónde puedo comer la mejor comida típica del valle, como empanadas o lulada\"\n",
    "resultados_3 = buscar_sitios_relevantes(consulta_3)\n",
    "print(f\"\\n--- Resultados para la consulta: '{consulta_3}' ---\")\n",
    "for res in resultados_3:\n",
    "    print(f\"  - Puntuación: {res['score']:.4f} | Nombre: {res['nombre']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:11:06,085 - INFO - API de Google Gemini configurada exitosamente.\n",
      "E0000 00:00:1759813866.087855  103006 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759813866.087855  103006 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API de Google Gemini configurada exitosamente.\n",
      "\n",
      "--- Modelos de Gemini Disponibles para tu API Key ---\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-robotics-er-1.5-preview\n"
     ]
    }
   ],
   "source": [
    "# [6]\n",
    "# --- Configuración y DIAGNÓSTICO de la API de Google Gemini ---\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = os.path.join(os.getcwd(), '../../.env/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        logging.info(\"API de Google Gemini configurada exitosamente.\")\n",
    "        print(\"API de Google Gemini configurada exitosamente.\")\n",
    "\n",
    "        # --- ¡ESTA ES LA PARTE IMPORTANTE PARA DEPURAR! ---\n",
    "        # Listamos todos los modelos que soportan 'generateContent' para tu clave API.\n",
    "        print(\"\\n--- Modelos de Gemini Disponibles para tu API Key ---\")\n",
    "        for m in genai.list_models():\n",
    "            if 'generateContent' in m.supported_generation_methods:\n",
    "                print(m.name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"No se pudo configurar la API de Gemini. Error: {e}\")\n",
    "        print(f\"Error crítico al configurar la API de Gemini. Verifica tu clave API. Error: {e}\")\n",
    "else:\n",
    "    logging.warning(\"No se encontró la variable de entorno 'GOOGLE_API_KEY'.\")\n",
    "    print(\"Advertencia: No se encontró la clave API de Google.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7]\n",
    "# --- Integración con el Generador (Google Gemini Pro - Corregido Definitivamente) ---\n",
    "\n",
    "def generar_respuesta_con_gemini(query: str, contexto: list):\n",
    "    \"\"\"\n",
    "    Toma la consulta original y el contexto recuperado para generar una respuesta usando Gemini.\n",
    "    \"\"\"\n",
    "    if not GOOGLE_API_KEY:\n",
    "        return \"Error: La clave API de Google no está configurada. No se puede generar una respuesta.\"\n",
    "    if not contexto:\n",
    "        return \"Lo siento, no pude encontrar ninguna recomendación relevante para tu consulta en nuestra base de datos.\"\n",
    "\n",
    "    # 1. Instanciar el modelo con el alias 'latest' que SÍ está en tu lista de modelos.\n",
    "    # Corregimos de 'gemini-1.5-pro-latest' a 'gemini-pro-latest'.\n",
    "    llm_model = genai.GenerativeModel('gemini-pro-latest')\n",
    "    \n",
    "    # 2. Construir el prompt detallado para el LLM (esto no cambia)\n",
    "    prompt = f\"\"\"\n",
    "    Eres \"Cali-Guía IA\", un asistente de turismo experto, amigable y apasionado por la ciudad de Cali, Colombia.\n",
    "    Tu objetivo es ayudar a un turista a planificar su visita basándote ÚNICAMENTE en la información que te proporciono.\n",
    "\n",
    "    La pregunta del turista es: \"{query}\"\n",
    "\n",
    "    He encontrado los siguientes lugares que parecen ser muy relevantes. Úsalos como base para tu respuesta:\n",
    "    \"\"\"\n",
    "    for i, sitio in enumerate(contexto):\n",
    "        prompt += f\"\\n--- Contexto del Lugar {i+1} ---\\n\"\n",
    "        prompt += f\"Nombre del Lugar: {sitio['nombre']}\\n\"\n",
    "        prompt += f\"Ubicación: {sitio['direccion']}\\n\"\n",
    "        prompt += f\"Extracto de opiniones de visitantes: {sitio['comentarios'][:400]}...\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "    ---\n",
    "    Tu Tarea:\n",
    "    1.  Escribe una respuesta conversacional y atractiva. Empieza con un saludo cálido.\n",
    "    2.  Recomienda los lugares del contexto que mejor se ajusten a la pregunta del turista.\n",
    "    3.  Para cada lugar recomendado, menciona su nombre y por qué es una buena opción, basándote en las opiniones o características proporcionadas.\n",
    "    4.  NO inventes información. Si no tienes detalles sobre algo (ej. horarios, precios exactos), no los menciones.\n",
    "    5.  Termina con una frase amigable, como \"¡Espero que disfrutes tu increíble visita a Cali!\".\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Hacer la llamada a la API y manejar posibles errores (esto no cambia)\n",
    "    try:\n",
    "        logging.info(f\"Generando respuesta con el modelo '{llm_model.model_name}'...\")\n",
    "        response = llm_model.generate_content(prompt)\n",
    "        logging.info(\"Respuesta recibida de la API de Gemini.\")\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ocurrió un error al llamar a la API de Gemini: {e}\")\n",
    "        return f\"Lo siento, hubo un problema al comunicarme con el asistente de IA. Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 00:11:06,920 - INFO - Recibida nueva consulta: 'Tengo una cena importante con mi pareja, busco un sitio elegante, con buena comida y un ambiente especial'\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
      "2025-10-07 00:11:07,293 - INFO - Búsqueda completada. Se encontraron 3 resultados relevantes.\n",
      "2025-10-07 00:11:07,296 - INFO - Generando respuesta con el modelo 'models/gemini-pro-latest'...\n",
      "\n",
      "2025-10-07 00:11:07,293 - INFO - Búsqueda completada. Se encontraron 3 resultados relevantes.\n",
      "2025-10-07 00:11:07,296 - INFO - Generando respuesta con el modelo 'models/gemini-pro-latest'...\n",
      "E0000 00:00:1759813867.301317  103006 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759813867.301317  103006 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "2025-10-07 00:11:30,056 - INFO - Respuesta recibida de la API de Gemini.\n",
      "2025-10-07 00:11:30,056 - INFO - Respuesta recibida de la API de Gemini.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================================\n",
      "========= EJECUCIÓN DEL FLUJO RAG COMPLETO =========\n",
      "========================================================\n",
      "\n",
      "CONSULTA DE USUARIO:\n",
      "'Tengo una cena importante con mi pareja, busco un sitio elegante, con buena comida y un ambiente especial'\n",
      "\n",
      "CONTEXTO RECUPERADO (lo que ve el LLM):\n",
      "  1. restaurante casa mar (Score: 0.722)\n",
      "  2. asados temos (Score: 0.705)\n",
      "  3. afuego mgico (Score: 0.696)\n",
      "\n",
      "RESPUESTA GENERADA POR GEMINI PRO:\n",
      "¡Hola! Soy Cali-Guía IA, ¡qué bueno poder ayudarte a encontrar el lugar perfecto en mi querida Cali para esa cena tan especial! Una velada romántica en la Sucursal del Cielo es un plan maravilloso.\n",
      "\n",
      "Basándome en lo que buscas, aquí te presento las opciones que mejor se ajustan para crear un momento inolvidable:\n",
      "\n",
      "1.  **afuego mgico:** Este lugar parece ser exactamente lo que tienes en mente. Según las opiniones de otros visitantes, es **\"un lugar hermoso y especial para compartir con tu pareja\"**. Es ideal si lo que quieres es sorprenderla con una **\"cena espectacular\"**. Además, destacan que tiene **\"un buen servicio y comida muy buena\"**, ¡elementos clave para tu noche!\n",
      "\n",
      "2.  **restaurante casa mar:** Esta es otra fantástica opción que podría encantarte. Quienes lo han visitado lo describen como **\"un lugar muy bonito, con una deliciosa sazón y muy buena atención\"**. El hecho de que una pareja lo haya elegido para celebrar su aniversario juntos sugiere que es un ambiente perfecto para una ocasión romántica como la tuya.\n",
      "\n",
      "Espero que esta información te ayude a planear una velada inolvidable.\n",
      "\n",
      "¡Espero que disfrutes tu increíble visita a Cali\n"
     ]
    }
   ],
   "source": [
    "# [8]\n",
    "# --- Ejecución del Flujo RAG Completo con Gemini ---\n",
    "\n",
    "consulta_final = \"Tengo una cena importante con mi pareja, busco un sitio elegante, con buena comida y un ambiente especial\"\n",
    "\n",
    "# Paso 1: Recuperar (Retrieve) - Esto no cambia\n",
    "contexto_recuperado = buscar_sitios_relevantes(consulta_final, top_k=3)\n",
    "\n",
    "# Paso 2: Generar (Generate) - Ahora llama a nuestra nueva función\n",
    "respuesta_final = generar_respuesta_con_gemini(consulta_final, contexto_recuperado)\n",
    "\n",
    "\n",
    "print(\"\\n\\n========================================================\")\n",
    "print(\"========= EJECUCIÓN DEL FLUJO RAG COMPLETO =========\")\n",
    "print(\"========================================================\")\n",
    "print(f\"\\nCONSULTA DE USUARIO:\\n'{consulta_final}'\\n\")\n",
    "print(\"CONTEXTO RECUPERADO (lo que ve el LLM):\")\n",
    "for i, sitio in enumerate(contexto_recuperado):\n",
    "    print(f\"  {i+1}. {sitio['nombre']} (Score: {sitio['score']:.3f})\")\n",
    "\n",
    "print(\"\\nRESPUESTA GENERADA POR GEMINI PRO:\")\n",
    "print(respuesta_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
